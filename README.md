# Opening the black box of Deep Neural Networks via Information: reimplementation

Group project by Leonid Matyushin, Alexander Lyzhov, Alexey Ustimenko, Philip Blagoveschensky, Nikolay Skuratov

### Code parts

* [Testing different step size policies](step_size_policies.ipynb)

* [Exploring a minimalistic 3-neuron setup](Minimal.ipynb)

* [Tanh/ReLU information planes](tanh_relu_comparison.ipynb)

* [Compression of irrelevant information](commpression_of_irrel_information.ipynb)

### Resources used

Papers and discussions:

* [Opening the black box of Deep Neural Networks via Information](https://arxiv.org/abs/1703.00810)

* [On the Information Bottleneck Theory of Deep Learning](https://openreview.net/pdf?id=ry_WPG-A-)

* [Openreview discussion of opponent paper](https://openreview.net/forum?id=ry_WPG-A-)

* [Blogpost critiquing the original paper](https://severelytheoretical.wordpress.com/2017/09/28/no-information-bottleneck-probably-doesnt-open-the-black-box-of-deep-neural-networks/)

* Acolyer blog discussions: [1](https://blog.acolyer.org/2017/11/15/opening-the-black-box-of-deep-neural-networks-via-information-part-i/), [2](https://blog.acolyer.org/2017/11/24/on-the-information-bottleneck-theory-of-deep-learning/)

* [Webema blogpost](https://weberna.github.io/jekyll/update/2017/11/08/Information-Bottleneck-Part1.html)

Code:

* [Original implementation](https://github.com/ravidziv/IDNNs)

* [Simplified implementation](https://github.com/stevenliuyi/information-bottleneck)

* [Opponent paper implementation](https://github.com/artemyk/ibsgd)

